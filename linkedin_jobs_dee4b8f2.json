{
  "jobs": [
    {
      "title": "Software Engineer",
      "company": "hackajob",
      "location": "Bengaluru, Karnataka, India",
      "url": "https://in.linkedin.com/jobs/view/software-engineer-at-hackajob-4307991209",
      "post_date": "2025-10-02",
      "scraped_at": "2025-10-21T13:45:22.344596",
      "source": "public_api",
      "details": {
        "description": "hackajob\n is collaborating with \nUnitedHealth Group\n to connect them with exceptional tech professionals for this role.\nOptum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start\n Caring. Connecting. Growing together. \nWe are seeking a highly skilled and experienced Senior Java Developer to join our dynamic team. The ideal candidate will have solid expertise in Java development, with a focus on Spring Boot, Apache Kafka, and database technologies such as MySQL or other relational/non-relational databases. You will be responsible for designing, developing, and maintaining scalable backend solutions that support our business objectives.\nPrimary Responsibilities\n \nSoftware Development:\nDesign, develop, and maintain scalable, high-performance backend systems using Java and Spring Boot\nImplement event-driven architectures using Apache Kafka to ensure real-time data streaming and integration\nDevelop RESTful APIs and microservices that support front-end applications and business processes\nDatabase Management:\nDesign and optimize database schemas, queries, and indexes in MySQL or other relational databases\nWork with NoSQL databases (e.g., MongoDB, Cassandra, etc.) if required\nEnsure data integrity, security, and performance optimization for database operations\nSystem Design & Architecture:\nCollaborate with architects and other developers to design system architecture that aligns with business requirements\nImplement best practices for microservices architecture and distributed systems\nIntegration & Deployment:\nIntegrate systems using messaging platforms such as Apache Kafka for data streaming and communication\nImplement CI/CD pipelines and ensure smooth deployment processes\nTroubleshooting & Maintenance:\nDebug, troubleshoot, and resolve complex technical issues in a timely manner\nPerform code reviews to ensure high-quality code and adherence to industry best practices\nCollaboration:\nWork closely with cross-functional teams, including product managers, QA engineers, and DevOps teams, to deliver high-quality software solutions\nMentor junior developers and contribute to the overall growth of the technical team\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\nRequired Qualifications\nHands-on experience with Apache Kafka for event-driven systems and data streaming\nExperience with version control systems like Git\nKnowledge of containerization and orchestration tools such as Docker and Kubernetes\nSolid knowledge of relational databases, especially MySQL (including query optimization, indexing, etc.)\nFamiliarity with microservices architecture and RESTful API development\nExpertise in Spring Boot and the Spring ecosystem\nSolid proficiency in Java programming language \nProven solid problem-solving and analytical skills\nProven excellent communication and interpersonal skills\nProven ability to work independently and as part of a team\nProven ability to mentor and guide junior developers\nPreferred Qualifications\nExperience with NoSQL databases (e.g., MongoDB, Redis) \nExposure to cloud platforms (e.g., AWS, Azure, GCP)\nAt UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone-of every race, gender, sexuality, age, location and income-deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes - an enterprise priority reflected in our mission.\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "Redis",
          "MongoDB",
          "Azure",
          "AWS",
          "Kafka",
          "Java",
          "Spring Boot",
          "Kubernetes",
          "Cassandra",
          "GCP",
          "Docker",
          "MySQL"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Software Engineer - Python",
      "company": "PayPal",
      "location": "Bengaluru, Karnataka, India",
      "url": "https://in.linkedin.com/jobs/view/software-engineer-python-at-paypal-4287141862",
      "post_date": "2025-08-26",
      "scraped_at": "2025-10-21T13:45:22.344596",
      "source": "public_api",
      "details": {
        "description": "The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nWe offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do – and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Description Summary:\nThis job implements tasks within the Software Development Lifecycle (SDLC), follows internal conventions and procedures, makes technical decisions, collaborates with peers and project leads, and contributes to code and design reviews.\nJob Description:\nJob Description\nYour way to impact\nAt PayPal, Software Engineers are the architects of our global payment platform.\n You'll design, develop, and optimize core systems that power millions of transactions daily, directly impacting our customers' experiences and our company's success.\nYour day-to-day\nAs a \nSoftware Engineer - Python\n you'll contribute to building robust backend systems. You'll collaborate closely with experienced engineers to learn and grow your skills.\nDevelop and maintain backend components.\nWrite clean, efficient code adhering to coding standards.\nParticipate in code reviews and provide feedback.\nWhat Do You Need To Bring\n2+ years of backend development experience. Strong foundation in programming concepts and data structures and bachelor’s degree in computer science or related field.\nProficiency in Python programming language.\nProficiency in backend development using Python frameworks, with experience in technologies such as frameworks like Django and Flask.\nStrong understanding of web services and Service-Oriented Architecture (SOA) standards\nExperience with databases (SQL, NoSQL)\nPreferred Qualifications\nAdditional Responsibilities and Preferred Qualifications\nExperience with large-scale, high-performance systems.\nKnowledge of the payment processing industry and relevant regulations.\nExperience with cloud platforms (AWS, GCP, Azure).\nContributions to open-source projects.\nPayPal does not charge candidates any fees for courses, applications, resume reviews, interviews, background checks, or onboarding. Any such request is a red flag and likely part of a scam. To learn more about how to identify and avoid recruitment fraud please visit https://careers.pypl.com/contact-us.\nFor the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nAt PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset-you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com\nWho We Are:\nTo learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community.\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don’t hesitate to apply.\nREQ ID R0129215\n        \n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "Azure",
          "AWS",
          "Django",
          "Python",
          "GCP",
          "Flask"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Software Engineer, Machine Learning",
      "company": "LinkedIn",
      "location": "Bengaluru, Karnataka, India",
      "url": "https://in.linkedin.com/jobs/view/software-engineer-machine-learning-at-linkedin-4304003122",
      "post_date": "2025-10-14",
      "scraped_at": "2025-10-21T13:45:22.344596",
      "source": "public_api",
      "details": {
        "description": "Company Description\nLinkedIn is the worlds largest professional network, built to create economic opportunity for every member of the global workforce. Our products help people make powerful connections, discover exciting opportunities, build necessary skills, and gain valuable insights every day. Were also committed to providing transformational opportunities for our own employees by investing in their growth. We aspire to create a culture thats built on trust, care, inclusion, and fun where everyone can succeed.\nJob Description\nJoin the LSS-AI team and help create meaningful impact by connecting sellers and buyers to mutual growth opportunities! LinkedIn Sales Solutions (LSS) is one of the fastest-growing multi-billion dollar verticals at LinkedIn, and the LSS-AI team is at the forefront of building the next generation of AI-centric Sales Navigator platform. This platform enables tens of millions of companies to connect with their future customers, driving significant economic value. As a member of the LSS-AI team, you will work on cutting-edge projects, including building Agentic and GenAI-powered search and recommendation systems, tuning and applying domain-specific LLMs, and collaborating with our partner vendors like OpenAI. You will have the opportunity to work with passionate AI engineers, as well as our brilliant cross-functional partner teams in Engineering, Product Management, and Design. Together, we are creating immense economic value for our customers using the largest global professional economic graph.\nAs part of a new and fast growing team of top-notch scientists and engineers, you will experience all the excitement and dynamism of a startup along with the scale and technology of a world-class enterprise.\nAt LinkedIn, our approach to flexible work is centered on trust and optimized for culture, connection, clarity, and the evolving needs of our business. The work location of this role is hybrid, meaning it will be performed both from home and from a LinkedIn office on select days, as determined by the business needs of the team.\nResponsibilities\nDevelop advanced embedding models (text, graph, structured data) to capture semantic meaning of companies, people, and signals for retrieval and ranking.\nDesign and train ranking models that optimize search relevance, entity discovery, and lead recommendations across millions of entities.\nBuild large-scale classification models to automatically tag industries, technologies, buyer intent, and other business signals.\nDeploy and monitor production ML pipelines for embedding generation, model inference, and continuous retraining with real-world feedback loops.\nDevelop state-of-the-art supervised and semi-supervised models that scale to hundreds of millions of companies, professionals, and business signals.\nOwn end-to-end model development and deployment of embeddings, retrieval, ranking, and classification models in large-scale production environments.\nMentor junior ML engineers in applying advanced machine learning techniques to core problems such as entity resolution, lead scoring, and search relevance.\nRepresent LinkedIn in academic and industry circles by showcasing our innovation, data products and scientific expertise in bringing game-changing data products to market\nQualifications\nBasic Qualifications\nBachelor's degree or related practical experience\n2+ years of experience in at least one of the following areas: Recommendation systems, Machine Learning, Statistical modeling/inference, Data mining, Graph/geometric deep learning, Large Language Models, Generative AI, Distributed Training.\nPreferred Qualifications\nMaster's degree in computer science, statistics, engineering, mathematics, or similar quantitative discipline.\n3+ years of hands-on experience working on Recommendation systems, Machine Learning, Statistical modeling/inference, Data mining, Graph/geometric deep learning, Large Language Models, Generative AI, Distributed Training.\nUnderstanding of standard programming and software engineering practices\nProven track record of publishing at academic and industry venues\nAbility and eagerness to program\nSuggested Skills\nMachine Learning\nGenerative-AI\nLarge Language Models\nYou will Benefit from our Culture\nWe strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels.\nAdditional Information\nIndia Disability Policy \nLinkedIn is an equal employment opportunity employer offering opportunities to all job seekers, including individuals with disabilities. For more information on our equal opportunity policy, please visit https://legal.linkedin.com/content/dam/legal/Policy_India_EqualOppPWD_9-12-2023.pdf\nGlobal Data Privacy Notice for Job Candidates \nPlease follow this link to access the document that provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://legal.linkedin.com/candidate-portal.\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Analyst",
      "company": "Unilever",
      "location": "Bengaluru, Karnataka, India",
      "url": "https://in.linkedin.com/jobs/view/data-analyst-at-unilever-4315079254",
      "post_date": "2025-10-16",
      "scraped_at": "2025-10-21T13:45:22.344596",
      "source": "public_api",
      "details": {
        "description": "JOB TITLE: \nData Analyst\nJOB FUNCTION: \nR&D\nSCOPE: \nGlobal\nWORK LOCATION: \nUnilever Research India – R&D Bangalore, Whitefield\nAbout Unilever\nWith 3.4 billion people in over 190 countries using our products every day, Unilever is a business that makes a real impact on the world. Work on brands that are loved and improve the lives of our consumers and the communities around us. We are driven by our purpose: to make sustainable living commonplace, and it is our belief that doing business the right way drives superior performance. At the heart of what we do is our people – we believe that when our people work with purpose, we will create a better business and a better world.\nAt Unilever, your career will be a unique journey, grounded in our inclusive, collaborative, and flexible working environment. We don’t believe in the ‘one size fits all’ approach and instead we will equip you with the tools you need to shape your own future.\nCategory Or Function Introduction\nHome Care is a global business with leading household cleaning and laundry brands such as OMO, Sunlight and Comfort. Our aim is to offer products that are unmissably superior, sustainable and great value.\nWe are organised to deliver growth and margin across our core formats. While we have a portfolio of strong global brands, and a global geographical footprint, our strength is in emerging markets where we lead the industry through market development.\nWithin R&D, our team of world-leading scientists, researchers and professionals creates innovations that drive growth for our business and deliver positive impacts for people, society and our planet.\nJob Purpose\nThe purpose of this role is to make a stronger pillar in digital for data capture as a future requirement. Unilever has opted for various data capture tools and are active for users across functions.\nWhat Will Your Main Responsibilities Be\nConvert non-technical requirements of lab testing procedures in technical and build digital templates in the R&D recommended tools such as LIMS (Unilever), ELN etc for Formulation, Stability, Processing and Packaging, working alongside with R&D SMEs\nDemonstrate LIMS functionalities for creating requests, batches for Lab, pilot trails and MPT/ 3P as well as for Material and Inventory management modules\nBuild digital visualization solutions to demonstrate power of data in R&D\nSupport R&D in building scope of their digital data capture projects and in upskilling knowledge.\nWhat You Need To Succeed\nExperiences & Qualifications\nBachelor’s or master’s degree in STEM subject (Science / Engineering) \nMinimum 2-3 years of working experience in the field of data handling and processing.\nKnowing / working experience in LIMS, ELN systems would be reasonable.\nSkills\nInnovative\nManaging stakeholders. Should be able to translate non -tech requirements to technical\nPower BI working knowledge is plus.\nOur commitment to Equality, Diversity & Inclusion \nUnilever embraces diversity and encourages applicants from all walks of life! This means giving full and fair consideration to all applicants and continuing development of all employees regardless of age, disability, gender reassignment, race, religion or belief, sex, sexual orientation, marriage and civil partnership, and pregnancy and maternity.\n\"All official offers from Unilever are issued only via our Applicant Tracking System (ATS). Offers from individuals or unofficial sources may be fraudulent—please verify before proceeding.\"\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "Power BI",
          "R"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer I",
      "company": "Amazon",
      "location": "Bengaluru, Karnataka, India",
      "url": "https://in.linkedin.com/jobs/view/data-engineer-i-at-amazon-4300576316",
      "post_date": "2025-10-06",
      "scraped_at": "2025-10-21T13:45:22.344596",
      "source": "public_api"
    },
    {
      "title": "Data Analyst",
      "company": "Deloitte",
      "location": "Bengaluru, Karnataka, India",
      "url": "https://in.linkedin.com/jobs/view/data-analyst-at-deloitte-4303479098",
      "post_date": "2025-09-25",
      "scraped_at": "2025-10-21T13:45:22.344596",
      "source": "public_api",
      "details": {
        "description": "Data Analyst\nLocation:\n Bangalore / Pune\nAbout the Role:\nWe are looking for an experienced Data Analyst with strong technical expertise and a solution-oriented mindset. The ideal candidate will leverage advanced skills in Python, PySpark, and SQL to manage and transform data, uncover insights, and support complex business use cases. Experience in the banking sector and knowledge of data governance principles will be a strong advantage.\nKey Responsibilities:\nAnalyze and transform large datasets using PySpark and SQL to solve complex business problems.\nIdentify data anomalies and perform thorough data validation to ensure accuracy and quality.\nCollaborate effectively with stakeholders, managing expectations and delivering data-driven solutions.\nTake ownership of the end-to-end data development and deployment process.\nUphold data governance, quality, privacy, and control standards throughout the data lifecycle.\nTechnical Skills & Experience:\nMinimum 5 years of professional experience in Python programming.\n5-7 years of hands-on experience with PySpark for data management and transformation.\n3-5 years of experience with SQL, including advanced query writing and optimization.\nFamiliarity with version control tools such as Git.\nStrong understanding of data governance, data quality, controls, and privacy practices.\nPreferred Skills:\nExperience in the banking or financial services domain.\nKnowledge of data pipeline building and workflow orchestration (e.g., Prophecy).\nAbility to model data and design scalable data architectures.\nProven stakeholder management and communication skills.\nIf you’re passionate about data, enjoy tackling complex challenges, and want to make an impact in a dynamic environment, we’d love to hear from you!\nKindly apply on this given link,\nhttps://forms.office.com/e/v8D1peAFxP\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "Python"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer I",
      "company": "Amazon",
      "location": "Bengaluru, Karnataka, India",
      "url": "https://in.linkedin.com/jobs/view/data-engineer-i-at-amazon-4300564911",
      "post_date": "2025-10-06",
      "scraped_at": "2025-10-21T13:45:22.344596",
      "source": "public_api"
    },
    {
      "title": "Data Analyst",
      "company": "Curefit",
      "location": "Bengaluru, Karnataka, India",
      "url": "https://in.linkedin.com/jobs/view/data-analyst-at-curefit-4311927826",
      "post_date": "2025-10-14",
      "scraped_at": "2025-10-21T13:45:22.344596",
      "source": "public_api",
      "details": {
        "description": "Curefit Healthcare Pvt Ltd, founded in 2016, is India’s largest fitness company and wellness platform. Cult, from the house of Curefit, was established with a mission to make fitness easy, fun and accessible to everyone, and has emerged as a community celebrating the joy of fitness. The brand today offers a range of fitness services ranging from group workouts, gyms and personalized fitness solutions, as well as an expansive selection of quality fitness products for the everyday athlete. The company’s vision is to empower everyone towards an active and healthier lifestyle through innovative fitness solutions, enabled by technology.\nAs a Sr Executive in our Contact Center Control Tower, you will be central to ensuring smooth and efficient operations. You'll be responsible for managing MIS, Workforce Management (WFM), and Real-Time Management (RTM) functions. Your key focus will be on monitoring real-time performance, analyzing data to identify trends and issues, deriving actionable insights, and driving course correction to optimize our sales and customer support teams.\nRoles & Responsibilities\nMIS & Reporting: Develop and maintain key performance reports and dashboards. Ensure data accuracy for decision-making.\nWorkforce Management (WFM): Oversee scheduling, forecasting, and capacity planning to meet operational needs.\nReal-Time Management (RTM): Monitor live contact center activity and agent performance. Identify and address immediate operational challenges.\nReal-Time Monitoring: Continuously track key metrics and agent adherence to schedules. Flag deviations and escalate as needed.\nData Analysis & Insights: Analyze performance data to identify trends, root causes of issues, and opportunities for improvement.\nCourse Correction: Based on real-time observations and data analysis, recommend and implement immediate adjustments and long-term solutions to enhance efficiency and KPIs.\nCollaboration: Work closely with operations managers, team leaders, and other stakeholders to ensure alignment and effective strategy execution.\nQualifications:\nBachelor's degree in a relevant field.\nProven experience (typically 3+ years) in contact center operations, directly involved in MIS, WFM, and/or RTM.\nStrong analytical and problem-solving skills.\nProficiency in data analysis tools (e.g., Excel, Google Sheets, and SQL is a plus) and contact center technology.\nAbility to interpret data, identify trends, and translate findings into actionable steps.\nExcellent communication and collaboration skills.\nUnderstanding of key contact center metrics and KPIs.\nAbility to work effectively in a fast-paced, real-time environment.\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Software Engineer, Systems Infrastructure",
      "company": "LinkedIn",
      "location": "Bengaluru, Karnataka, India",
      "url": "https://in.linkedin.com/jobs/view/software-engineer-systems-infrastructure-at-linkedin-4315355742",
      "post_date": "2025-10-15",
      "scraped_at": "2025-10-21T13:45:22.344596",
      "source": "public_api",
      "details": {
        "description": "Company Description\nLinkedIn is the worlds largest professional network, built to create economic opportunity for every member of the global workforce. Our products help people make powerful connections, discover exciting opportunities, build necessary skills, and gain valuable insights every day. Were also committed to providing transformational opportunities for our own employees by investing in their growth. We aspire to create a culture thats built on trust, care, inclusion, and fun where everyone can succeed.\nJob Description\nThis role will be based in Bangalore, India.\nAt LinkedIn, our approach to flexible work is centered on trust and optimized for culture, connection, clarity, and the evolving needs of our business. The work location of this role is hybrid, meaning it will be performed both from home and from a LinkedIn office on select days, as determined by the business needs of the team.\nAs part of our world-class software engineering team, you will be charged with building the next-generation infrastructure and platforms for LinkedIn, including but not limited to: an application and service delivery platform, massively scalable data storage and replication systems, cutting-edge search platform, best-in-class AI platform, experimentation platform, privacy and compliance platform etc. You will work and learn among the best, putting to use your passion for distributed technologies and algorithms, API design and systems-design, and your passion for writing code that performs at an extreme scale. LinkedIn has already pioneered well-known open-source infrastructure projects like Apache Kafka, Pinot, Azkaban, Samza, Venice, Datahub, Feather, etc. We also work with industry standard open source infrastructure products like Kubernetes, GRPC and GraphQL - come join our infrastructure teams and share the knowledge with a broader community while making a real impact within our company.\nResponsibilities\nYou will design, build and operate one of the online data infra platforms that power all of Linkedin's core applications.\nYou will participate in design and code reviews to maintain our high development Standards.\nYou will partner with peers, leads and partners to define, scope, prioritize, and build impactful features at a high velocity.\nQualifications\nBasic Qualifications\nBA/BS Degree in Computer Science or related technical discipline, or related practical experience\n2+ years of industry experience in software design, development, and algorithm related solutions\nExperience programming in Object-oriented languages such as Java, Python, Go, and/or Functional languages such as Scala\nPreferred Qualifications\nExperience in building machine learning platforms at large scale\nFamiliarity with techniques used in big data and AI\nGood knowledge in distributed data processing technologies\nExperience with industry, open-source projects and/or academic research in large-data, machine learning systems, such as Spark, TensorFlow, PyTorch, or XGBoost\nSuggested Skills\nDistributed systems\nBackend Systems Infrastructure\nJava\nYou will Benefit from our Culture\nWe strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels.\nAdditional Information\nIndia Disability Policy \nLinkedIn is an equal employment opportunity employer offering opportunities to all job seekers, including individuals with disabilities. For more information on our equal opportunity policy, please visit https://legal.linkedin.com/content/dam/legal/Policy_India_EqualOppPWD_9-12-2023.pdf\nGlobal Data Privacy Notice for Job Candidates \nPlease follow this link to access the document that provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://legal.linkedin.com/candidate-portal.\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "TensorFlow",
          "Java",
          "Python",
          "Kubernetes",
          "PyTorch",
          "Scala",
          "Spark",
          "Go",
          "Kafka"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer – Financial Infrastructure & Analytics",
      "company": "Meril",
      "location": "Bangalore Urban, Karnataka, India",
      "url": "https://in.linkedin.com/jobs/view/data-engineer-%E2%80%93-financial-infrastructure-analytics-at-meril-4316488880",
      "post_date": null,
      "scraped_at": "2025-10-21T13:45:22.344596",
      "source": "public_api",
      "details": {
        "description": "About the Role\nAs a \nQuantitative Data Engineer\n, you will be the backbone of the data ecosystem powering our \nquantitative research, trading, and AI-driven strategies\n. You will design, build, and maintain the \nhigh-performance data infrastructure\n that enables low-latency, high-fidelity access to market, fundamental, and alternative data across multiple asset classes.\nThis role bridges \nquant engineering, data systems, and research enablement\n, ensuring that our researchers and traders have fast, reliable, and well-documented datasets for analysis and live trading. You’ll be part of a cross-functional team working at the intersection of \nfinance, machine learning, and distributed systems\n.\nResponsibilities\nArchitect and maintain scalable ETL pipelines\n for ingesting and transforming terabytes of structured, semi-structured, and unstructured market and alternative data.\nDesign time-series optimized data stores\n and \nstreaming frameworks\n to support low-latency data access for both backtesting and live trading.\nDevelop ingestion frameworks\n integrating vendor feeds (Bloomberg, Refinitiv, Polygon, Quandl, etc.), exchange data, and internal execution systems.\nCollaborate with quantitative researchers and ML teams\n to ensure data accuracy, feature availability, and schema evolution aligned with modeling needs.\nImplement data quality checks, validation pipelines, and version control mechanisms\n for all datasets.\nMonitor and optimize distributed compute environments\n (Spark, Flink, Ray, or Dask) for performance and cost efficiency.\nAutomate workflows\n using orchestration tools (Airflow, Prefect, Dagster) for reliability and reproducibility.\nEstablish best practices\n for metadata management, lineage tracking, and documentation.\nContribute to internal libraries and SDKs\n for seamless data access by trading and research applications.\nIn Trading Firms, Data Engineers Typically:\nBuild \nreal-time data streaming systems\n to capture market ticks, order books, and execution signals.\nManage \nversioned historical data lakes\n for backtesting and model training.\nHandle \nmulti-venue data normalization\n (different exchanges and instruments).\nIntegrate \nalternative datasets\n (satellite imagery, news sentiment, ESG, supply-chain data).\nWork closely with \nquant researchers\n to convert raw data into \nresearch-ready features\n.\nOptimize pipelines for \nultra-low latency\n where milliseconds can impact P&L.\nImplement \ndata observability frameworks\n to ensure uptime and quality.\nCollaborate with \nDevOps and infra engineers\n to scale storage, caching, and compute.\nTech Stack\nLanguages:\n Python, SQL, Scala, Go, Rust (optional for HFT pipelines)\nData Processing:\n Apache Spark, Flink, Ray, Dask, Pandas, Polars\nWorkflow Orchestration:\n Apache Airflow, Prefect, Dagster\nDatabases & Storage:\n PostgreSQL, ClickHouse, DuckDB, ElasticSearch, Redis\nData Lakes:\n Delta Lake, Iceberg, Hudi, Parquet\nStreaming:\n Kafka, Redpanda, Pulsar\nCloud & Infra:\n AWS (S3, EMR, Lambda), GCP, Azure, Kubernetes\nVersion Control & Lineage:\n DVC, MLflow, Feast, Great Expectations\nVisualization / Monitoring:\n Grafana, Prometheus, Superset, DataDog\nTools for Finance:\n kdb+/q (for tick data), InfluxDB, QuestDB\nWhat You Will Gain\nEnd-to-end ownership\n of core data infrastructure in a high-impact, mission-critical domain.\nDeep exposure to \nquantitative research workflows\n, \nmarket microstructure\n, and \nreal-time trading systems\n.\nCollaboration with elite quantitative researchers, traders, and ML scientists.\nHands-on experience with \ncutting-edge distributed systems\n and \ntime-series data technologies\n.\nA culture that emphasizes \ntechnical excellence, autonomy, and experimentation.\nQualifications\nBachelor’s or Master’s in \nComputer Science, Data Engineering, or related field.\n2+ years\n of experience building and maintaining \nproduction-grade data pipelines\n.\nProficiency in \nPython\n, \nSQL\n, and frameworks like \nAirflow\n, \nSpark\n, or \nFlink\n.\nFamiliarity with \ncloud storage and compute (S3, GCS, EMR, Dataproc)\n and \nversioned data lakes (Delta, Iceberg)\n.\nExperience with \nfinancial datasets\n, \ntick-level data\n, or \nhigh-frequency time series\n is a strong plus.\nStrong understanding of \ndata modeling, schema design, and performance optimization\n.\nExcellent communication skills with an ability to support \nmultidisciplinary teams\n.\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "Redis",
          "AWS",
          "Azure",
          "PostgreSQL",
          "Python",
          "Kubernetes",
          "ElasticSearch",
          "Scala",
          "Spark",
          "Go",
          "Airflow",
          "GCP",
          "Kafka"
        ],
        "industry": "Not specified"
      }
    }
  ],
  "metadata": {
    "scraped_at": "2025-10-21T13:46:21.266803",
    "total_jobs": 10,
    "total_with_details": 8
  }
}