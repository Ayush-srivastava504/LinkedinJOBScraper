{
  "jobs": [
    {
      "title": "Data Engineer (L5)",
      "company": "Netflix",
      "location": "United States",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-l5-at-netflix-3982462774",
      "post_date": "2025-10-13",
      "scraped_at": "2025-10-21T13:32:34.984345",
      "source": "public_api",
      "details": {
        "description": "Netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.\nNetflix is revolutionizing how shows and movies are produced, pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet, and continuously improving the end-to-end user experience with Netflix across their member journey.\nWe pride ourselves on using data to inform our decision-making as we work towards our mission. This requires curating data across various domains such as Ads, Games, Growth, Finance, Product, Content, and Studio. All of this data collection and curation is made possible thanks to the amazing Data Engineers of Netflix who bring this data to life.\nData Engineering at Netflix is a role that requires building systems to process data efficiently and modeling the data to power analytics. These solutions can range from batch data pipelines that bring business metrics to life to real-time processing services that integrate with our core product features. In addition, we require our Data Engineers to have a rich understanding of large distributed systems on which our data solutions rely. Candidates should have knowledge across several of these skill sets and usually need to be deep in at least one. As a Data Engineer, you also need to have strong communication skills since you will need to collaborate with business, engineering, and data science teams to enable a culture of learning. Learn more about the work of data engineers at Netflix.\nLocation of work\n: We are considering candidates who are willing to relocate to Los Gatos, California, as well as fully-remote candidates (remote in the US with occasional visits to Los Gatos) depending on the team your skills are most aligned with.\nWho are you?\nYou strive to write elegant code, and you're comfortable with picking up new technologies independently\nYou are proficient in at least one major programming language (e.g. Java, Scala, Python) and comfortable working with SQL\nYou enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine learning models\nYou have a strong background in at least one of the following: distributed data processing or software engineering of data services, or data modeling\nYou are familiar with big data technologies like Spark or Flink and comfortable working with web-scale datasets\nYou have an eye for detail, good data intuition, and a passion for data quality\nYou appreciate the importance of great documentation and data debugging skills\nYou relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback\nYou are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risks\nOur compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for is $170,000 - $720,000.\nNetflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.\nNetflix is a unique culture and environment. Learn more here.\nInclusion is a Netflix value and we strive to host a meaningful interview experience for all candidates. If you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner.\nWe are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\nJob is open for no less than 7 days and will be removed when the position is filled.\n        \n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "Java",
          "Python",
          "Scala",
          "Spark"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer (L4) - Platform",
      "company": "Netflix",
      "location": "United States",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-l4-platform-at-netflix-4302933782",
      "post_date": "2025-10-15",
      "scraped_at": "2025-10-21T13:32:34.984345",
      "source": "public_api"
    },
    {
      "title": "Data Engineer",
      "company": "SKIMS",
      "location": "Los Angeles, CA",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-skims-4315898642",
      "post_date": "2025-10-15",
      "scraped_at": "2025-10-21T13:32:34.984345",
      "source": "public_api"
    },
    {
      "title": "Junior Data Engineer",
      "company": "Cylinder",
      "location": "United States",
      "url": "https://www.linkedin.com/jobs/view/junior-data-engineer-at-cylinder-4317522771",
      "post_date": null,
      "scraped_at": "2025-10-21T13:32:34.984345",
      "source": "public_api"
    },
    {
      "title": "Senior Data Engineer - Data Science",
      "company": "LinkedIn",
      "location": "Sunnyvale, CA",
      "url": "https://www.linkedin.com/jobs/view/senior-data-engineer-data-science-at-linkedin-4315355747",
      "post_date": "2025-10-15",
      "scraped_at": "2025-10-21T13:32:34.984345",
      "source": "public_api",
      "details": {
        "description": "Company Description\nLinkedIn is the worlds largest professional network, built to create economic opportunity for every member of the global workforce. Our products help people make powerful connections, discover exciting opportunities, build necessary skills, and gain valuable insights every day. Were also committed to providing transformational opportunities for our own employees by investing in their growth. We aspire to create a culture thats built on trust, care, inclusion, and fun where everyone can succeed.\nJob Description\nLinkedIn's Data Science team leverages big data to empower business decisions and deliver data-driven insights, metrics, and tools in order to drive member engagement, business growth, and monetization efforts. With over 1 billion members around the world, a focus on great user experience, and a mix of B2B and B2C programs, LinkedIn offers countless ways for an ambitious data engineer to have an impact and transform your career.\nWe are now looking for a talented and driven individual to accelerate our efforts and be a major part of our data-centric culture. This person will work closely with various cross-functional teams such as product, marketing, sales, engineering, and operations to develop infrastructure and deliver tools or data structures that enable data-driven decision-making. Successful candidates will exhibit technical acumen and business savviness with a passion for making an impact by enabling both producers and consumers of data insight to work smarter.\nAt LinkedIn, our approach to flexible work is centered on trust and optimized for culture, connection, clarity, and the evolving needs of our business. The work location of this role is hybrid, meaning it will be performed both from home and from a LinkedIn office on select days, as determined by the business needs of the team.\nResponsibilities\nWork with a team of high-performing data science professionals, and cross-functional teams to identify business opportunities and build scalable data solutions.\nBuild data expertise, act like an owner for the company and manage complex data systems for a product or a group of products.\nPerform all of the necessary data transformations to serve products that empower data-driven decision making.\nBuild and manage data pipelines, design and architect databases.\nEstablish efficient design and programming patterns for engineers as well as for non-technical partners.\nDesign, implement, integrate and document performant systems or components for data flows or applications that power analysis at a massive scale.\nEnsure best practices and standards in our data ecosystem are shared across teams.\nUnderstand the analytical objectives to make logical recommendations and drive informed actions.\nEngage with internal platform teams to prototype and validate tools developed in-house to derive insight from very large datasets or automate complex algorithms.\nBe a self-starter, Initiate and drive projects to completion with minimal guidance.\nContribute to engineering innovations that fuel LinkedIn's vision and mission.\nQualifications\nBasic Qualifications\nBachelor's Degree in a quantitative discipline: Computer science, Statistics, Operations Research, Informatics, Engineering, Applied Mathematics, Economics, etc.\n3+ years of relevant industry or relevant academia experience working with large amounts of data\nExperience with SQL/Relational databases\nBackground in at least one programming languages (e.g., R, Python, Java, Scala, PHP, JavaScript)\nPreferred Qualifications\nBS and 5+ years of relevant work experience, MS and 3+ years of relevant work experience, or Ph.D. and 1+ years of relevant work/academia experience working with large amounts of data\nMS or PhD in a quantitative discipline: statistics, operations research, computer science, informatics, engineering, applied mathematics, economics, etc.\nExperience in developing data pipelines using Spark and Hive.\nExperience with data modeling, ETL (Extraction, Transformation & Load) concepts, and patterns for efficient data governance. Experience with manipulating massive-scale structured and unstructured data.\nExperience with using distributed data systems such as Spark and related technologies (Presto/Trino, Hive, etc.).\nExperience with either data workflows/modeling, front-end engineering, or back-end engineering.\nDeep understanding of technical and functional designs for relational and MPP Databases\nExperience in data visualization and dashboard design including tools such as Tableau, R visualization packages, streamlit, D3, and other libraries, etc.\nKnowledge of Unix and Unix-like systems, version control systems such as Git.\nSuggested Skills\nDistributed Systems\nETL\nData Modeling\nYou will Benefit from our Culture\nWe strongly believe in the well-being of our employees and their families. That is why we offer generous health and wellness programs and time away for employees of all levels LinkedIn is committed to fair and equitable compensation practices. The pay range for this role is $125,000 to $206,000. Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location. This may be different in other locations due to differences in the cost of labor. The total compensation package for this position may also include annual performance bonus, stock, benefits and/or other applicable incentive compensation plans. For more information, visit https://careers.linkedin.com/benefits.\nAdditional Information\nEqual Opportunity Statement \nWe seek candidates with a wide range of perspectives and backgrounds and we are proud to be an equal opportunity employer. LinkedIn considers qualified applicants without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.\nLinkedIn is committed to offering an inclusive and accessible experience for all job seekers, including individuals with disabilities. Our goal is to foster an inclusive and accessible workplace where everyone has the opportunity to be successful.\nIf you need a reasonable accommodation to search for a job opening, apply for a position, or participate in the interview process, connect with us at accommodations@linkedin.com and describe the specific accommodation requested for a disability-related limitation.\nReasonable accommodations are modifications or adjustments to the application or hiring process that would enable you to fully participate in that process. Examples of reasonable accommodations include but are not limited to:\nDocuments in alternate formats or read aloud to you\nHaving interviews in an accessible location\nBeing accompanied by a service dog\nHaving a sign language interpreter present for the interview\nA request for an accommodation will be responded to within three business days. However, non-disability related requests, such as following up on an application, will not receive a response.\nLinkedIn will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by LinkedIn, or (c) consistent with LinkedIn's legal duty to furnish information.\nSan Francisco Fair Chance Ordinance \nPursuant to the San Francisco Fair Chance Ordinance, LinkedIn will consider for employment qualified applicants with arrest and conviction records.\nPay Transparency Policy Statement \nAs a federal contractor, LinkedIn follows the Pay Transparency and non-discrimination provisions described at this link: https://lnkd.in/paytransparency.\nGlobal Data Privacy Notice for Job Candidates \nPlease follow this link to access the document that provides transparency around the way in which LinkedIn handles personal data of employees and job applicants: https://legal.linkedin.com/candidate-portal.\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "JavaScript",
          "Java",
          "Python",
          "Tableau",
          "Scala",
          "Spark",
          "PHP",
          "R"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer (L5) - Games",
      "company": "Netflix",
      "location": "United States",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-l5-games-at-netflix-4313842380",
      "post_date": "2025-10-18",
      "scraped_at": "2025-10-21T13:32:34.984345",
      "source": "public_api",
      "details": {
        "description": "Netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.\nGames are our next big frontier and an incredible opportunity for us to deliver new experiences to delight and entertain our quickly growing membership. You will be joining us at an exciting time as we roll out more games on both mobile and cloud and be in a position to help us redefine what a Netflix subscription means for our members around the world.\nThe Data Science and Engineering (DSE) team uses data, analytics, and sciences to improve various aspects of our business. As a \nSenior Data Engineer \nfocused on \nCore Metrics and Metadata\n, you will be a foundational partner, leveraging high-quality data to support end-to-end analytics needs for our Games partners.\nWhat You'll Do\nAs a Senior Data Engineer, your focus will be on architecting the foundation for all business measurements across our games portfolio on all platforms.\nOwn Core Metric Systems: Design, develop, and maintain the single source of truth for core business and operational metrics (e.g., DAU/MAU, retention, engagement rates). Ensure metric consistency, explainability, and reliability for high-stakes business decisions.\nData Modeling Architecture: Create advanced analytical data models (facts, dimensions, aggregations) optimized for performance, partitioning, and backfill considerations.\nImplement High-Reliability Pipelines: Build and optimize robust, high-volume data pipelines using distributed processing frameworks like Spark, Flink, Python, and Scala. Master complex processing patterns (batch, CDC, incremental loads, upserts) and address challenges like event-time partitioning, late data handling, and windowing for real-time applications.\nData Governance and Quality Leadership: Define and enforce data quality standards (accuracy, completeness, consistency, validity) at scale. Implement data lineage tracking and monitor metadata to provide the necessary context for interpreting core metrics.\nStrategic Collaboration: Partner with Games Product, Data Science, and Engineering stakeholders to translate their forecasting, research, and analytical needs into scalable data solutions. Collaborate on strategies for metric collection, dashboarding, and integration with analytical tools.\nTechnical Leadership: Proactively identify and resolve technical debt, increase automation, and champion best practices in data engineering and software development across the team. Mentor junior team members and help raise the technical bar for the entire DSE organization.\nWho You Are\nWe value people over process and look for humbly confident, action-oriented collaborators with deep technical expertise.\nExperience: 7+ years of hands-on experience in software development with a deep focus on building high-performance data systems that collect, process, and govern high-volume telemetry data for analytics.\nTechnical Expertise (The Core Domain):\nExpert-level Data Modeling: Proven mastery of telemetry (event schema design) and analytical modeling (dimensional modeling, aggregation strategies, partitioning considerations).\nDistributed Processing Mastery: Demonstrated experience building production-grade data pipelines using distributed processing frameworks (e.g., Spark, Flink, Hive/Hadoop), with a strong understanding of distributed systems and performance optimization.\nAdvanced Data Processing: Direct experience with complex batch and streaming data processing patterns, including CDC, incremental loads, event-driven architectures, and robust error handling.\nProgramming & SQL: Expert-level programming proficiency in Python and/or Scala/Java and mastery of SQL. You maintain a strong software engineering mindset.\nPartnership and Leadership:\nStrong Partnership Skills: Exceptional communication skills and a proven ability to collaborate effectively with non-technical stakeholders to ensure metrics are accessible, reliable, and actionable.\nProven ability to drive and own complex, cross-functional engineering projects with a high degree of autonomy.\nHumble confidence and the awareness to recognize when to course-correct, valuing continuous learning and improvement.\nOur compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.\nNetflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.\nInclusion is a Netflix value and we strive to host a meaningful interview experience for all candidates. If you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner.\nWe are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\nJob is open for no less than 7 days and will be removed when the position is filled.\n        \n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "Java",
          "Python",
          "Scala",
          "Spark",
          "Hadoop"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer II",
      "company": "Microsoft",
      "location": "Redmond, WA",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-ii-at-microsoft-4316568842",
      "post_date": "2025-10-18",
      "scraped_at": "2025-10-21T13:32:34.986349",
      "source": "public_api",
      "details": {
        "description": "With continued growth in digital data and the desire to leverage data to measure in-production quality and address problems that touch all aspects of our lives, Microsoft’s Windows Servicing & Delivery Org is looking for an equally data- and quality-minded engineer to meet these challenges! Join the Update Platform team for the chance to have an impact on billions of customers every day. The Update Platform Team is responsible for ensuring the seamless delivery and integration of software updates and keeping our customers up-to-date and secure at all times.\nAs a \nData Engineer II\n member of the Update Platform Insights team, you will be at the forefront of leveraging data to assess the quality of the product, detect issues before they reach broad customer application to assure top product quality for partners and customers alike while keeping billions of devices secure and up-2-date.\nIn this exciting role, you'll work with a diverse group of talented professionals, innovate for greater platform efficiency as well as leveraging the latest technologies and best practices to streamline our update processes with timely in-depth insights and intelligent features.\nMicrosoft’s mission is to empower every person and every organization on the planet to achieve more. As employees we come together with a growth mindset, innovate to empower others, and collaborate to realize our shared goals. Each day we build on our values of respect, integrity, and accountability to create a culture of inclusion where everyone can thrive at work and beyond.\nResponsibilities\nData Management and Transformation: With guidance, you will apply modification techniques to transform raw data into compatible formats for downstream systems. Utilize software and computing tools to ensure data quality and completeness. Implement code to extract and validate raw data from upstream sources, ensuring accuracy and reliability.\nDrive Customer Success: Through Data and Business Insight: You will play a pivotal role in building a metrics-driven culture that directly impacts product quality and customer outcomes. This role goes beyond technical execution—you will design and implement measurement frameworks from the ground up while applying a strategic, top-down perspective to ensure the right metrics are in place. Your ability to translate data into actionable insights, aligned with business priorities and rhythm of business, will enable informed decisions that drive high-quality product outcomes and measurable customer success.\nData Requirements and Modeling: Collaborate with stakeholders to document and understand data requirements. Evaluate project plans to assess data costs, access, and availability. Draft design specifications to model data flow and storage, ensuring data is easy to connect and manage.\nCompliance: You will follow data modeling and handling procedures to maintain compliance with all applicable laws and policies across your assigned workstreams. You’ll also learn about permissions and approvals for data access within a data pipeline.\nValidation and Quality Mindset: Apply and use operational fundamentals to validate and ensure quality of the product as well as the underlying data pipeline and assets to secure trustworthiness in your data daily.\nCustomer Focus: Be driven by a focus on customer happiness and success. We as a team only succeed if our customers are secure and protected via the updates we deliver.\nEmbody our  Culture  and  Values.\nQualifications\nRequired Qualifications:\nMaster's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field AND 1+ year(s) experience in business analytics, data science, software development, data modeling, or data engineering\nOR Bachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field AND 2+ years experience in business analytics, data science, software development, data modeling, or data engineering \nOR equivalent experience.\n3+ years with scripting and coding languages with a focus on data engineering, like SQL, KQL, python, Scope, C# (or similar object-oriented languages) and others.\n1+ years of experience with building large data processing frameworks using technologies like Azure Data Factory, Azure Data Explorer, PowerBI and/or other public and Microsoft internal tools.\n1+ years of experience in analytics to define, monitor, and optimize key performance indicators (KPIs) and connected business metrics that ensure measurable customer success.\n1+ years of proven ability to orchestrate and sustain a data-driven rhythm of business, transforming insights into actionable strategies that align with organizational priorities and deliver impactful outcomes.\nOther Requirements\nAbility to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include but are not limited to the following specialized security screenings:\nMicrosoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.\nPreferred Qualifications\nA solid quality mindset with the ability to deliver end-to-end data solutions that build partner and customer confidence, ensuring alignment with business objectives and measurable outcomes.\nExperience with Git, ADO or equivalent Source Control Systems.\nExperience with data visualization tools and how to effectively communicate Insights to consumers of varying types of audiences.\nExperience leveraging AI to define and evaluate quality standards\nData Engineering IC3 - The typical base pay range for this role across the U.S. is USD $100,600 - $199,000 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $131,400 - $215,400 per year.\nCertain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here: https://careers.microsoft.com/us/en/us-corporate-pay \nMicrosoft will accept applications for the role until October 14, 2025.\n#WSDJobs\n#QualityThroughData\n#OKR\n#MetricsWithImpact\n#HighImpact\nMicrosoft is an equal opportunity employer. Consistent with applicable law, all qualified applicants will receive consideration for employment without regard to age, ancestry, citizenship, color, family or medical care leave, gender identity or expression, genetic information, immigration status, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran or military status, race, ethnicity, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable local laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application process, read more about requesting accommodations.\n        \n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "python",
          "Azure"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Software  Engineer, Data Visualization",
      "company": "OpenAI",
      "location": "San Francisco, CA",
      "url": "https://www.linkedin.com/jobs/view/software-engineer-data-visualization-at-openai-4207339818",
      "post_date": "2025-10-12",
      "scraped_at": "2025-10-21T13:32:34.986349",
      "source": "public_api",
      "details": {
        "description": "About The Team\nThe Data Visualization team at OpenAI is responsible for building and maintaining all the visualization tools used for analyzing various software and hardware aspects of our custom-built hyperscale supercomputers. This includes visualizing hardware (nodes, network, racks, etc.), monitoring how a user’s job is running on the platform, and assessing the health of the underlying systems. These tools allow us to analyze, improve, and operate the platform for running and training the world’s largest AI models. We work at the cutting edge of speed and scale, combining the traditions of High-Performance Computing (HPC) with a modern cloud and containerized environment.\nOur team is incubated within OpenAI’s Research team, operating at the forefront of AI innovations. The Platform Visualization team complements the existing platform teams that ensure our researchers are minimally impacted by hardware faults. We maximize available supercomputing capacity for researchers and maintain the reliability, scalability, and user-friendliness of job lifecycle management, with an emphasis on efficient job scheduling, quota management, and job execution workflows.\nAbout The Role\nAs a Software Engineer on the Platform Visualization team, you will play a critical role in designing, developing, and maintaining the full-stack visualization tools that are essential for analyzing the software and hardware aspects of OpenAI’s hyperscale supercomputers. Your work will involve creating intuitive front-end interfaces and back-end systems for visualizing hardware components, monitoring training job performance on the platform, and ensuring the health of underlying systems.\nIn this role, you will collaborate closely with other engineering and research teams to gather requirements, understand visualization needs, and deliver full-stack solutions that enhance our ability to analyze, improve, and operate the platform.\nKey Responsibilities\nDevelop and maintain full-stack visualization tools for hardware and software analysis.\nDesign intuitive front-end interfaces and robust back-end systems for monitoring the performance and health of supercomputer systems.\nCollaborate with researchers and engineers to understand their needs and deliver effective full-stack visualization solutions.\nEnsure high performance, reliability, and scalability of visualization tools across both front-end and back-end systems.\nContinuously improve existing tools and develop new features to meet evolving requirements.\nQualifications\nStrong experience in full-stack software development, with a focus on building scientific or infrastructure visualization tools.\nProficiency in both front-end and back-end programming languages such as Python, JavaScript, SQL, or similar.\nFamiliar with front-end technologies like React and back-end technologies like Node.js, and databases like Snowflake.\nExperience with visualization libraries and frameworks (e.g., Plotly, Grafana).\nStrong understanding of full-stack architecture, design principles, and best practices.\nExcellent problem-solving skills and attention to detail.\nStrong communication skills and the ability to work collaboratively in a team environment.\nBonus: Prior experience technically leading a team of 4+ engineers, as this is a 0-1 effort with team growth on the horizon\nBonus if familiar with High-Performance Computing (HPC) environments and modern cloud/container technologies (e.g., Kubernetes, Azure).\nThis role offers the opportunity to work on some of the largest and most advanced AI infrastructure in the world, directly contributing to the success of OpenAI and the advancement of the field of AI. If you are passionate about cutting-edge technology and eager to tackle complex challenges, we would love to hear from you\nAbout OpenAI\nOpenAI is an AI research and deployment company dedicated to ensuring that general-purpose artificial intelligence benefits all of humanity. We push the boundaries of the capabilities of AI systems and seek to safely deploy them to the world through our products. AI is an extremely powerful tool that must be created with safety and human needs at its core, and to achieve our mission, we must encompass and value the many different perspectives, voices, and experiences that form the full spectrum of humanity.\nWe are an equal opportunity employer, and we do not discriminate on the basis of race, religion, color, national origin, sex, sexual orientation, age, veteran status, disability, genetic information, or other applicable legally protected characteristic.\nFor additional information, please see OpenAI’s Affirmative Action and Equal Employment Opportunity Policy Statement.\nQualified applicants with arrest or conviction records will be considered for employment in accordance with applicable law, including the San Francisco Fair Chance Ordinance, the Los Angeles County Fair Chance Ordinance for Employers, and the California Fair Chance Act. For unincorporated Los Angeles County workers: we reasonably believe that criminal history may have a direct, adverse and negative relationship with the following job duties, potentially resulting in the withdrawal of a conditional offer of employment: protect computer hardware entrusted to you from theft, loss or damage; return all computer hardware in your possession (including the data contained therein) upon termination of employment or end of assignment; and maintain the confidentiality of proprietary, confidential, and non-public information. In addition, job duties require access to secure and protected information technology systems and related data security obligations.\nTo notify OpenAI that you believe this job posting is non-compliant, please submit a report through this form. No response will be provided to inquiries unrelated to job posting compliance.\nWe are committed to providing reasonable accommodations to applicants with disabilities, and requests can be made via this link.\nOpenAI Global Applicant Privacy Policy\nAt OpenAI, we believe artificial intelligence has the potential to help people solve immense global challenges, and we want the upside of AI to be widely shared. Join us in shaping the future of technology.\nCompensation Range: $255K - $405K\n        \n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "JavaScript",
          "Azure",
          "Kubernetes",
          "Python",
          "React",
          "Snowflake",
          "Node.js"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Software Engineer, Fullstack, Early Career",
      "company": "Notion",
      "location": "New York, NY",
      "url": "https://www.linkedin.com/jobs/view/software-engineer-fullstack-early-career-at-notion-4296641686",
      "post_date": "2025-10-18",
      "scraped_at": "2025-10-21T13:32:34.986349",
      "source": "public_api",
      "details": {
        "description": "About Us\nNotion helps you build beautiful tools for your life’s work. In today's world of endless apps and tabs, Notion provides one place for teams to get everything done, seamlessly connecting docs, notes, projects, calendar, and email—with AI built in to find answers and automate work. Millions of users, from individuals to large organizations like Toyota, Figma, and OpenAI, love Notion for its flexibility and choose it because it helps them save time and money.\nIn-person collaboration is essential to Notion's culture. We require all team members to work from our offices on Mondays and Thursdays, our designated Anchor Days. Certain teams or positions may require additional in-office workdays.\nAbout The Role\nAs an Early Career Software Engineer at Notion, you’ll help shape core user experiences and accelerate how people discover value in Notion. You'll tackle meaningful challenges with increasing autonomy, crafting code that millions of users will experience. You'll take ownership of projects that matter, make critical technical decisions, and contribute your unique perspective to our product vision. Working alongside passionate experts across design, product, and data, you'll help shape the future of how people work.\nWe’re hiring early career engineers to join us across several teams at Notion. Your recruiter will partner with you to find the team that best aligns with your interests and where you can make the highest impact.\nWhat You’ll Achieve\nPlan, build, and ship product features from conception to launch, then iterate based on insights and user feedback.\nImprove performance, reliability, and quality of key experiences used by millions of users and thousands of organizations.\nRun experiments that drive activation, retention, collaboration, and revenue, partnering with design, data science, and research.\nBuild internal tools and platform improvements that help all engineers ship quickly and safely.\nContribute to team norms, code quality, and a culture of learning and thoughtful tradeoffs.\nAreas you might work on\nCore Notion product features: Help develop, improve, and maintain features for our flagship product, focusing on the editor, databases, sharing, or other key areas that power millions of workflows.\nAI and automation: Help develop and integrate intelligent features that make Notion more powerful, contextual, and efficient for our users.\nGrowth and activation: Work on features that help new users discover Notion's value, improve onboarding experiences, and increase user engagement.\nPlatform and infrastructure: Contribute to the systems that power Notion's backend services, ensuring reliability, scalability, and performance as our user base grows.\nSkills You’ll Bring\nProven track record of execution: You have a minimum of 1 year (and up to 3 years) of full-time professional engineering experience, including building world-class product experiences as part of an engineering team. You have solid fundamentals in data structures, algorithms, and distributed systems, with a product-minded, pragmatic approach to solving problems.\nThoughtful problem-solving: You approach problems holistically, starting with a clear and accurate understanding of the context. You think about the implications of what you're building and how it will impact real people's lives. You can navigate ambiguity successfully, decompose complex problems into clean solutions, while also balancing the business impact of what you’re building.\nImpact-driven approach to technology: You see technologies as tools to achieve user impact rather than ends in themselves. You care more about building successful systems that solve real problems than about using specific tech stacks or following trends. You stay current with the latest tools like Cursor, Claude Code, and other AI-assisted development environments, you're pragmatic about choosing the right tool for the job, focusing on what delivers the most value to users and the business.\nProactive communication and high agency: You own your work, communicating clearly about progress and blockers. You don't wait for instructions for every step but rather show initiative in identifying what needs to be done and driving projects forward. You ask questions when needed while independently finding solutions to problems.\nNice to Have\nExperience with parts of our stack like React, TypeScript, Node.js, Postgres, or with experimentation and analytics tooling.\nExposure to distributed systems, observability, CI/CD, or infrastructure fundamentals.\nAn interest in product quality and craft, and in helping others stay in flow.\nYou've heard of computing pioneers like Ada Lovelace, Douglas Engelbart, Alan Kay, and others—and understand why we're big fans of their work.\nWe hire talented and passionate people from a variety of backgrounds because we want our global employee base to represent the wide diversity of our customers. If you’re excited about a role but your past experience doesn’t align perfectly with every bullet point listed in the job description, we still encourage you to apply. If you’re a builder at heart, share our company values, and enthusiastic about making software toolmaking ubiquitous, we want to hear from you.\nNotion is proud to be an equal opportunity employer. We do not discriminate in hiring or any employment decision based on race, color, religion, national origin, age, sex (including pregnancy, childbirth, or related medical conditions), marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or other applicable legally protected characteristic. Notion considers qualified applicants with criminal histories, consistent with applicable federal, state and local law. Notion is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, please let your recruiter know.\nNotion is committed to providing highly competitive cash compensation, equity, and benefits. The compensation offered for this role will be based on multiple factors such as location, the role’s scope and complexity, and the candidate’s experience and expertise, and may vary from the range provided below. For roles based in San Francisco or New York City, the estimated base salary range for this role is $126,000 - $180,000 per year.\nBy clicking “Submit Application”, I understand and agree that Notion and its affiliates and subsidiaries will collect and process my information in accordance with Notion’s Global Recruiting Privacy Policy.\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "TypeScript",
          "Node.js",
          "React"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Software Engineer, Fullstack, Early Career",
      "company": "Notion",
      "location": "San Francisco, CA",
      "url": "https://www.linkedin.com/jobs/view/software-engineer-fullstack-early-career-at-notion-4296646591",
      "post_date": "2025-10-18",
      "scraped_at": "2025-10-21T13:32:34.987352",
      "source": "public_api"
    },
    {
      "title": "Data Engineer",
      "company": "SKIMS",
      "location": "Los Angeles Metropolitan Area",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-skims-4316092243",
      "post_date": "2025-10-17",
      "scraped_at": "2025-10-21T13:32:40.885053",
      "source": "public_api",
      "details": {
        "description": "We are looking for a Data Engineer to be a founding member of our Data and Analytics team. In this role you will be responsible for building the foundational data analytics, reporting, and data pipelines platforms that power data-driven decision making across the entire company. This role is also critical in building data products and dashboards to empower data-driven decision making across the organization.\nKey Responsibilities\nArchitect, maintain and optimize scalable data pipelines to deliver critical insights for Skims using Fivetran, DBT, Airflow, Snowflake, BigQuery and various AWS services.\nAssemble large and complex data sets that meet functional and non-functional business requirements.\nMaintain and contribute to our data quality efforts using DBT tests and Monte Carlo.\nCollaborate with data scientists, analysts and other stakeholders to understand data requirements and deliver tailored solutions.\nDocument technical designs, workflows and best practices to facilitate knowledge sharing across the data team and other engineering teams.\nSupport process failures when necessary and serve in an on-call rotation. \nSkills, Knowledge & Expertise\n3+ years of experience in a data engineering role, preferably working in a direct-to-consumer business.\nExcellent SQL skills and experience using Python to manipulate data and draw insights from large datasets required.\nExperience creating dashboards and reporting using visualization tools like Looker (preferred), Tableau, or PowerBI required.\nDemonstrated ability to independently plan and execute complex data engineering pipelines for large datasets.\nAbility to translate business strategies into technical requirements.\nCreative thinker, adept at understanding a wide breath of retail analytics use cases and identifying the data and analytics necessary to make them into a reality.\nDirect experience with the data platform stack currently in use at SKIMS highly desirable: Fivetran, AWS Lambdas, dbt, Snowflake, BigQuery, Looker, Airflow.\nDemonstrated success working in a fast-paced, swiftly changing startup environment.\nBA/BS in Math, Computer Science, Analytics, Data Science, or another quantitative field desirable.\nJob Benefits\nBenefits And Culture\n100% Company Paid Healthcare (medical, dental, vision) \nKind Body Fertility Benefits \n401(k) savings plan with up to 4% match \nUnlimited PTO \nFull Access to LinkedIn learning \nEmployee Discounts \nPerks (HQ Location)\nFree weekly catered lunch at HQ – M/W/Th \nDog-Friendly office \nFree Swag Giveaways \nAnnual Holiday Party \nAnnual Summer Party \nInvitations to pop-ups and other company events \nComplimentary daily office snacks and beverages \nCompensation: $125,000 - $143,000 / year\n        \n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "AWS",
          "Python",
          "Looker",
          "Tableau",
          "Airflow",
          "Snowflake"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Software Engineer, New Grad",
      "company": "Stripe",
      "location": "San Francisco, CA",
      "url": "https://www.linkedin.com/jobs/view/software-engineer-new-grad-at-stripe-4294691515",
      "post_date": "2025-10-15",
      "scraped_at": "2025-10-21T13:32:40.885053",
      "source": "public_api",
      "details": {
        "description": "Who we are\nAbout Stripe\nStripe is a financial infrastructure platform for businesses. Millions of companies—from the world’s largest enterprises to the most ambitious startups—use Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Our mission is to increase the GDP of the internet, and we have a staggering amount of work ahead. That means you have an unprecedented opportunity to put the global economy within everyone’s reach while doing the most important work of your career.\nAbout The Team\nStripe offers an interesting middle ground between joining a small startup and going to work for a multinational company. We have systems and processes optimized for early-career professionals. You will be in an environment that prizes rigor, discipline, and reliability—from our products, from our ways of working, and from our colleagues. You will learn how the best teams put together extremely reliable systems, and you will be responsible for operating these systems, with adequate backup when things get tough.\nWhat you’ll do\nAs a company, we believe in end-to-end ownership of projects. For any given project, we have one person on point. While they don't necessarily have to do all the work themselves, it's their job to make sure all the work gets done. We launch betas and prototypes as early as we can. This helps ensure that we're building what users actually want. We contribute back to the community, often by building things we think are cool and by releasing open-source software.\nResponsibilities\nWork on cross-functional projects, directly collaborating with other engineers\nGive meaningful feedback on code reviews and technical designs\nEnsure that the systems your team operates continue running well and can scale to meet the needs of our users\nBuild the skills to own a project from beginning to end, learning project management and technical leadership skills\nWho you are\nWe’re looking for someone who meets the minimum requirements to be considered for the role. If you meet these requirements, you are encouraged to apply. The preferred qualifications are a bonus, not a requirement.\nMinimum Requirements\nA Bachelor’s or Master’s degree in computer science or a directly related field, obtained by summer 2026, or equivalent work experience\nSome experience and familiarity with programming, either through side projects or classwork. We work mostly in Java, Ruby, JavaScript, Scala, and Go. We believe new programming languages can be learned if the fundamentals and general knowledge are present\nExperience from either previous internships or working collaboratively on multi-person coding projects (in a university or professional setting)\nAbility to learn unfamiliar systems and form an understanding of those systems, through independent research and working with a mentor and subject matter experts\nPreferred Qualifications\nOne or more areas of specialized knowledge balanced with general skills and knowledge, such as knowing more frontend technologies and, at a high level, how a service handles an HTTP request\nExperience in code review practices, and an understanding of how to safely update production systems\nFamiliarity with navigating and managing work in large code bases\nIn-office expectations\nOffice-assigned Stripes in most of our locations are currently expected to spend at least 50% of the time in a given month in their local office or with users. This expectation may vary depending on role, team and location. For example, Stripes in our Bucharest, Romania site have an 80% in-office expectation, and those in Stripe Delivery Center roles in Mexico City, Mexico and Bengaluru, India work 100% from the office. Also, some teams have greater in-office attendance requirements, to appropriately support our users and workflows, which the hiring manager will discuss. This approach helps strike a balance between bringing people together for in-person collaboration and learning from each other, while supporting flexibility when possible.\nPay and benefits\nThe annual US base salary range for this role is $122,100 - $134,400. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. This salary range may be inclusive of several career levels at Stripe and will be narrowed during the interview process based on a number of factors, including the candidate’s experience, qualifications, and location. Applicants interested in this role and who are not located in the US may request the annual salary range for their location during the interview process.\nAdditional benefits for this role may include: equity, company bonus or sales commissions/bonuses; 401(k) plan; medical, dental, and vision benefits; and wellness stipends.\n        \n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "Ruby",
          "JavaScript",
          "Java",
          "Scala",
          "Go"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer",
      "company": "Facebook",
      "location": "United States",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-facebook-4314044511",
      "post_date": "2025-10-13",
      "scraped_at": "2025-10-21T13:32:40.885053",
      "source": "public_api",
      "details": {
        "description": "Meta, W2 Contract\nData Engineer - Remote\nJob Summary:\nMeta is seeking a talented Data Engineer to join our team within the Edge Network Services division. This role will support the M360 project, focusing on data analytics and metrics. As part of the ENS Analytics team, you will enable data-driven decision-making, develop L1/L2/L3 metrics for key processes, and automate Opex and Capex tracking. Your responsibilities will include designing scalable data solutions, automating data pipelines, improving data quality, and establishing effective metrics governance to support ENS operations. Additionally, you will create dashboards, reports, and self-service analytics tools to empower ENS teams to monitor, analyze, and act on reliable data insights.\nJob Qualifications:\n5+ years of experience as a Data Engineer supporting company infrastructure, data analysis, and metrics\nProficiency in Linux and shell scripting\nExpertise in MySQL database administration, implementation, and maintenance\nStrong SQL skills or similar languages, with development experience in at least one scripting language (Python preferred)\nSolid understanding of data architecture, data modeling, and schema design\nPlus if having prior Meta experience\nJob Info:\nJob Type: Contract (W2)\nJob Location: Remote\nJob Duration: 1 year, plus 1 year extension (2 years)\nHourly Pay: $60.00-$70.00 per hour\nWork Hours: Monday-Friday / 40 hours per week\nFeatured Benefits:\nMedical insurance\nVision insurance\nDental insurance\n401(k) Plan\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "MySQL",
          "Python"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer Intern",
      "company": "Coinbase",
      "location": "New York, NY",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-intern-at-coinbase-4314425484",
      "post_date": "2025-10-13",
      "scraped_at": "2025-10-21T13:32:40.885053",
      "source": "public_api",
      "details": {
        "description": "Ready to be pushed beyond what you think you’re capable of?\nAt Coinbase, our mission is to increase economic freedom in the world. It’s a massive, ambitious opportunity that demands the best of us, every day, as we build the emerging onchain platform — and with it, the future global financial system.\nTo achieve our mission, we’re seeking a very specific candidate. We want someone who is passionate about our mission and who believes in the power of crypto and blockchain technology to update the financial system. We want someone who is eager to leave their mark on the world, who relishes the pressure and privilege of working with high caliber colleagues, and who actively seeks feedback to keep leveling up. We want someone who will run towards, not away from, solving the company’s hardest problems.\nOur work culture is intense and isn’t for everyone. But if you want to build the future alongside others who excel in their disciplines and expect the same from you, there’s no better place to be.\nWhile many roles at Coinbase are remote-first, we are not remote-only. In-person participation is required throughout the year. Team and company-wide offsites are held multiple times annually to foster collaboration, connection, and alignment. Attendance is expected and fully supported.\n \nThis is a 12-week internship during summer 2026.\nAs a Data Engineer Intern on the Strategy, Execution, & Analytics team, you'll collaborate closely with business partners in product and engineering to identify key opportunities and ensure accurate data interpretation. You'll also work with the analytics engineering team to build scalable analytics models and systems that broaden insights across the company and within our products. We're looking for someone who demonstrates clear communication, a strong drive to execute, and excellent craftsmanship.\nWhat you’ll be doing (ie. job duties):\nCollaborate with analytics engineering to design and implement robust data pipelines, ensuring data integrity and accessibility for analytical and machine learning initiatives.\nAssist in the development and maintenance of scalable data infrastructure, leveraging Python and cloud-based technologies.\nSupport senior engineers in integrating and managing data from various sources, including APIs.\nContribute to the implementation of LLM frameworks, focusing on efficient data flow and state machines.\nParticipate in analytical deep dives alongside senior team members to understand and address complex business problems, clearly communicating findings.\nLearn and apply best practices in data engineering, including data governance, security, and performance optimization.\nWhat we look for in you (ie. job requirements):\nExperience or coursework in programming, preferably in Python, with a strong understanding of data structures and algorithms.\nUnderstanding of data analysis and database concepts, with experience using SQL.\nFamiliarity with or strong interest in AI, LLMs, and API integrations.\nUnderstanding of software architecture principles and a keen interest in furthering this knowledge (in microservices, distributed systems, cloud-native design, etc.).\nAn embodiment of our core cultural values: clear communication, positive energy, continuous learning, and efficient execution.\nNice to haves:\nCurrently pursuing a degree or certificate in Computer Science, Data Engineering, or a related discipline.\nExposure to cloud platforms (e.g., AWS, GCP, Azure).\nEagerness to delve into complex data challenges and contribute to innovative solutions.\nJob #: P73387\n \nPay Transparency Notice: Depending on your work location, the target annual salary for this position can range as detailed below. Full time offers from Coinbase also include bonus eligibility + benefits (including medical, dental, vision and 401(k)).\nPay Range:: $50 USD - $50 USD\n \nPlease be advised that each candidate may submit a maximum of four applications within any 30-day period. We encourage you to carefully evaluate how your skills and interests align with Coinbase's roles before applying.\nCommitment to Equal Opportunity\nCoinbase is proud to be an Equal Opportunity Employer.  All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, sex, gender expression or identity, sexual orientation or any other basis protected by applicable law. Coinbase will also consider for employment qualified applicants with criminal histories in a manner consistent with applicable federal, state and local law.  For US applicants, you may view the Employee Rights and the Know Your Rights notices by clicking on their corresponding links. Additionally, Coinbase participates in the E-Verify program in certain locations, as required by law. \nCoinbase is also committed to providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the employment process, please contact us at accommodations[at]coinbase.com to let us know the nature of your request and your contact information.   For quick access to screen reading technology compatible with this site click here to download a free compatible screen reader (free step by step tutorial can be found here).\nGlobal Data Privacy Notice for Job Candidates and Applicants\nDepending on your location, the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) may regulate the way we manage the data of job applicants. Our full notice outlining how data will be processed as part of the application procedure for applicable locations is available here. \nBy submitting your application, you are agreeing to our use and processing of your data as required. For US applicants only, by submitting your application you are agreeing to arbitration of disputes as outlined here.   \nAI Disclosure\nFor select roles, Coinbase is piloting an AI tool based on machine learning technologies to conduct initial screening interviews to qualified applicants. The tool simulates realistic interview scenarios and engages in dynamic conversation. A human recruiter will review your interview responses, provided in the form of a voice recording and/or transcript, to assess them against the qualifications and characteristics outlined in the job description.  \nFor select roles, Coinbase is also piloting an AI interview intelligence platform to transcribe and summarize interview notes, allowing our interviewers to fully focus on you as the candidate. \nThe above pilots are for testing purposes and Coinbase will not use AI to make decisions impacting employment\n. To request a reasonable accommodation due to disability, please contact accommodations[at]coinbase.com\n \n \n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "Azure",
          "AWS",
          "Python",
          "GCP"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer Intern",
      "company": "Ibotta",
      "location": "Denver, CO",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-intern-at-ibotta-4305768884",
      "post_date": "2025-10-18",
      "scraped_at": "2025-10-21T13:32:40.885053",
      "source": "public_api",
      "details": {
        "description": "Ibotta is seeking a \nData Engineering Intern\n to join our innovative team and contribute to our mission to Make Every Purchase Rewarding. As a participant in our \n2026 Summer Internship\n, you will have the opportunity to fully integrate with your team to directly contribute to ongoing Ibotta business initiatives. In addition to the daily time with your team, you will participate in ongoing teach outs curated to help build skills essential to transitioning into the workforce and building your career. We are looking for candidates who are eager to learn, excited to work on real-world business challenges, and want to be part of a mission-driven company.\nThis will be a full-time, 12 week, internship during the summer of 2026. This is a hybrid position located in Denver, Colorado and requires 3 days in-office per week. Required in-office hybrid days are Tuesday, Wednesday, and Thursday. Candidates must live in the United States.\nWhat you will be doing:\nContribute as a member of the Data Infrastructure and Compliance Engineering squad to deliver data products and resolve data-related technical issues\nHelp build and manage automation tools and data pipelines that meet Data Governance and Data Security Standards\nAssist in the development and maintenance of Infrastructure as Code (IaC) templates (e.g., Terraform) to provision and manage cloud data resources\nUse Python and Scala to utilize Spark to collect and manage data at scale\nCollaborate with the team to identify, design, and implement process improvements, including automating manual processes, optimizing data delivery, and re-designing infrastructure for greater reliability and performance\nHelp to support the engineering of distributed systems, frameworks, and design patterns, enabling efficient usage of Ibotta’s Data Lake\nEvangelize Data Engineering and supporting capabilities with Platform and Analytics teams\nEmbrace and uphold Ibotta’s Core Values: Integrity, Boldness, Ownership, Teamwork, Transparency & A good idea can come from anywhere\nWhat we are looking for:\nJuniors working towards a bachelor’s degree with a focus in Computer Science, Engineering, Data Analytics or related field\nProven ability to think creatively and implement ideas from start to finish\nGood written and verbal communication skills\nHunger to learn and collaborate with your teammates\nPossess a strong work ethic\nAnalytical and problem-solving abilities\nSelf-directed and self-motivated\nAbout Ibotta (\"I bought a...\")\nIbotta (NYSE: IBTA) is a leading performance marketing platform allowing brands to deliver digital promotions to over 200 million consumers through a network of publishers called the Ibotta Performance Network (IPN). The IPN allows marketers to influence what people buy, and where and how often they shop – all while paying only when their campaigns directly result in a sale. American shoppers have earned over $1.8 billion through the IPN since 2012. The largest tech IPO in history to come out of Colorado, Ibotta is headquartered in Denver, and is continually listed as a top place to work by The Denver Post and Inc. Magazine.\nTo learn more about what our Tech teams are doing day to day, visit Building Ibotta on Medium.com\nAdditional Details:\nThis position is located in Denver, CO and includes competitive pay and housing assistance for out of state students. Denver office perks include snacks, and occasional meals.\nThe hourly compensation for this role is: $33.46 per hour worked. This compensation range is specific to the United States labor market.\nIbotta is an Equal Opportunity Employer. Ibotta’s employment decisions are made without regard of race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender identity, sexual orientation, or any other legally protected status.\nApplicants must be currently authorized to work in the United States on a full-time basis.\nApplicants are accepted until the position is filled.\nFor the security of our employees and the business, all employees are responsible for the secure handling of data in accordance with our security policies, identifying and reporting phishing attempts, as well as reporting security incidents to the proper channels.\n#BI-Hybrid\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "Scala",
          "Terraform",
          "Python",
          "Spark"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer",
      "company": "Phamily",
      "location": "United States",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-phamily-4306275986",
      "post_date": "2025-10-18",
      "scraped_at": "2025-10-21T13:32:40.885053",
      "source": "public_api",
      "details": {
        "description": "Healthcare - Data Engineer\nLocation:\n Remote\nJob Type:\n Full-time/Contract\nFull-time Salary Range: \n$110K - $140K\nContract Salary Range: \n$90-$135/hr\nAbout Us\nPhamily is a healthcare technology company focused on making proactive care management simple, scalable, and financially sustainable. Our SaaS platform equips health systems and physician groups with AI-powered virtual care tools to better manage chronic conditions and general patient needs between office visits. By shifting healthcare from reactive, episodic treatment to proactive care management, we’ve already supported tens of thousands of patients, and we’re just getting started. Join us as we continue to revolutionize how care is delivered.\nJob/Role Description\nAs a Data Engineer at Phamily, you’ll own the pipelines and infrastructure that power our AI-driven virtual care platform. You’ll build automated data workflows that ingest, standardize, and deliver healthcare data into our product and analytics environment. You’ll also support the operationalization of ML/AI tools and keep essential reference datasets accurate and up to date.\nThis role is highly impactful: the pipelines you design will ensure our teams have timely, trusted data to improve patient outcomes and support clinicians at scale. If you’re excited about building simple, reliable, and secure data systems in healthcare, this role is for you.\nKey Responsibilities\nDesign, build, and automate batch data pipelines that ingest from files, APIs, and SFTP into BigQuery and product environments.\nImplement scheduling, monitoring, retries, and backfills to ensure reliable and repeatable workflows.\nEstablish guardrails such as schema management, versioning, and basic SLAs for data freshness and reliability.\nProductionize ML/AI batch jobs and publish outputs into analytics-ready tables.\nMaintain and refresh healthcare reference datasets (e.g., NPI, codesets, CMS lists) on schedule.\nDocument pipelines clearly and make outputs consumable for analytics and BI teams.\nHandle PHI with care and follow HIPAA-aligned data governance practices.\nRequirements\n2+ years of experience building batch data workflows using Python and SQL, publishing to cloud data warehouses (BigQuery preferred).\nProficiency with modern schedulers/orchestrators (e.g., Airflow, Prefect, Dagster) and containerized environments.\nExperience ingesting data from APIs, files, and SFTP sources, including schema evolution management.\nStrong debugging, monitoring, and CI/CD fundamentals.\nExcellent documentation and communication skills with an ownership mindset.\nBonus: Experience with dbt, data quality testing, or healthcare data formats (claims, EHR, CMS datasets).\nBonus: Familiarity with running ML jobs in production.\nOur Compensation & Benefits\nCompetitive compensation commensurate with experience\nPotential to earn equity based on performance\nRemote-friendly work environment\nMedical, dental, and vision coverage for employees and dependents at a nominal cost\nPaid maternity leave\nFSA and Dependent Care account options\n401(k) eligibility after 6 months of full-time employment\nCollaborative, mission-driven work environment\nIf you take pride in delivering results, embrace challenges, and proactively seek improvement, then this is the place for you. You’ll join a smart, humble, and collaborative team dedicated to improving healthcare.\nEqual Employment Opportunity\nPhamily is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. Employment decisions are made without regard to race, color, religion, sex, national origin, age, disability, genetics, veteran status, sexual orientation, gender identity or expression, or any other legally protected status.\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "Python",
          "Airflow"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Senior Data Engineer",
      "company": "Gallup",
      "location": "San Francisco, CA",
      "url": "https://www.linkedin.com/jobs/view/senior-data-engineer-at-gallup-4311462691",
      "post_date": "2025-10-13",
      "scraped_at": "2025-10-21T13:32:40.885053",
      "source": "public_api",
      "details": {
        "description": "Engineer data systems that change how people live and work. \nAs a senior data engineer at Gallup, you’ll play a key role in designing, developing and optimizing the data systems that underpin our flagship platform, Gallup Access. You'll use your enterprise-level data technology expertise to enhance our data capabilities; modernize existing systems; expand GenAI; and ensure the performance, security and reliability of our data infrastructure. Working with technologies like AWS Aurora and other AWS storage and compute services, you’ll influence how we empower better decision-making by delivering reliable data and insights on everything that matters, from global wellbeing to employee engagement.\nAs part of our Technology team, you won't just maintain systems; you'll actively shape their future. If you excel at tackling complex data challenges and building robust, scalable solutions, join us in developing technology that truly changes the world.\nWhat You’ll Do\nDesign, develop and implement scalable data solutions, including AWS Aurora (MySQL compatible) and other cloud-native services\nOptimize data performance and design scalable data models that enable efficient storage, processing and real-time reporting solutions\nRearchitect and modernize existing data systems and ETL processes — migrating and transforming legacy pipelines to improve efficiency and maintainability\nEnhance data pipelines by integrating and leveraging Generative AI to improve automation, enrichment and insights\nDesign with foresight, creating system road maps, anticipating challenges and resolving issues before they impact performance\nImplement and maintain data security controls and ensure compliance with industry best practices\nWork with AWS services like S3, SQS, SNS, Lambda, Elasticache and DynamoDB\nCollaborate with application engineers, data scientists and other stakeholders to translate business needs into scalable data solutions\nMentor junior data engineers by offering guidance, proposing new approaches, providing feedback and sharing best practices\nTroubleshoot complex database issues, document resolutions and implement preventative measures\nContribute to data-related services and tooling using Python or Java as needed\nWhat Makes You Stand Out\nExpert problem-solving: With your deep understanding of data architecture and your experience working on complex transaction and analytics workloads, you diagnose performance bottlenecks and design efficient solutions.\nForward-thinking mindset: You anticipate future needs, advocate for modernization, and stay current with emerging data technologies and techniques. \nPerformance-oriented engineering: You’re committed to designing robust data models, improving the clarity and outcomes of systems, producing accurate reporting and analytics, implementing security best practices, and ensuring data integrity and compliance.\nCollaborative mentorship: You share knowledge generously, communicate technical concepts and steps clearly, and help junior engineers level up their skills and expertise.\nAgility and adaptability: You’re comfortable working across data systems and open to learning and contributing using various relevant technologies.\nWhat You Need\nBachelor’s degree in computer science, MIS or a related field, or equivalent experience, required\nAt least five years of experience in data engineering or backend systems with significant data responsibilities required\nAt least five years of experience in database design and development including data modeling, query optimization and performance tuning required\nAt least five years of experience with relational database management systems required, with MySQL experience preferred\nAt least two years of experience with cloud infrastructure required, with AWS experience preferred\nAt least two years of experience with ETL processes required\nAt least two years of experience with data warehousing, reporting and analytics required\nProfessional experience with Python or Java required\nKnowledge of data security principles and compliance best practices required\nExperience with data modernization projects preferred\nFamiliarity with cloud data warehousing solutions such as Snowflake is a plus\nA commitment to working on-site at Gallup’s San Francisco office at least three days a week required\nAbout Gallup\nAt Gallup, we change the world, one client at a time, through extraordinary analytics and advice on everything important facing humankind.\nGallup offers a robust benefits package that includes medical, dental, vision, life and other insurance options; a fully vested 401(k) retirement savings plan with company matching; an employee stock ownership program; mass transit reimbursement; family-building benefits; an employee assistance program; and various reimbursements and activities that enhance our associates’ wellbeing. We also offer an estimated annual salary range of $150,000-$200,000 for this role. Salaries are based on a variety of factors, including an individual’s education, experience and skills.\nGallup is an equal opportunity employer. We consider all qualified applicants without regard to race, color, religion, sex, national origin, disability, protected veteran status, sexual orientation, gender identity, or any other legally protected basis, in accordance with applicable law.\nTo review Gallup’s Privacy Statement, please click this link: https://www.gallup.com/privacy. This privacy policy is meant to help you understand what information we collect, why we collect it, and how you can update, manage and delete your information. Your application and the information you provide will be processed and stored in the United States.\n        \n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "AWS",
          "Java",
          "Python",
          "DynamoDB",
          "Snowflake",
          "MySQL"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer",
      "company": "iRocket",
      "location": "New Hyde Park, NY",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-irocket-4317060082",
      "post_date": "2025-10-20",
      "scraped_at": "2025-10-21T13:32:40.885053",
      "source": "public_api",
      "details": {
        "description": "Data is pivotal to our goal of frequent launch and rapid iteration. We're recruiting a Data Engineer at iRocket to build pipelines, analytics, and tools that support propulsion test, launch operations, manufacturing, and vehicle performance.\nThe Role\nDesign and build data pipelines for test stands, manufacturing machines, launch telemetry, and operations systems\nDevelop dashboards, real-time monitoring, data-driven anomaly detection, performance trending, and predictive maintenance tools\nWork with engineers across propulsion, manufacturing, and operations to translate data-needs into data-products\nMaintain data architecture, ETL processes, cloud/edge-data systems, and analytics tooling\nSupport A/B testing, performance metrics, and feed insights back into design/manufacturing cycles\nRequirements\nBachelor's degree in Computer Science, Data Engineering, or related technical field\n2+ years of experience building data pipelines, ETL/ELT workflows, and analytics systems\nProficient in Python, SQL, cloud data platforms (AWS, GCP, Azure), streaming/real-time analytics, and dashboarding (e.g., Tableau, PowerBI)\nStrong ability to work cross-functionally and deliver data-products to engineering and operations teams\nStrong communication, documentation, and a curiosity-driven mindset\nBenefits\nHealth Care Plan (Medical, Dental & Vision)\nRetirement Plan (401k, IRA)\nLife Insurance (Basic, Voluntary & AD&D)\nPaid Time Off (Vacation, Sick & Public Holidays)\nFamily Leave (Maternity, Paternity)\nShort Term & Long Term Disability\nWellness Resources\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "Azure",
          "AWS",
          "Python",
          "Tableau",
          "GCP"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer I",
      "company": "Tempus AI",
      "location": "Chicago, IL",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-i-at-tempus-ai-4304317235",
      "post_date": "2025-10-15",
      "scraped_at": "2025-10-21T13:32:40.885053",
      "source": "public_api",
      "details": {
        "description": "Passionate about precision medicine and advancing the healthcare industry?\nRecent advancements in underlying technology have finally made it possible for AI to impact clinical care in a meaningful way. Tempus' proprietary platform connects an entire ecosystem of real-world evidence to deliver real-time, actionable insights to physicians, providing critical information about the right treatments for the right patients, at the right time.\nWhat You’ll Do\nBuild, maintain and scale data pipelines and models that power analytics, reporting, and self-service exploration.\nCollaborate with BI engineers, product managers, and engineering teams to translate business needs into reliable data solutions.\nSupport data governance, including data quality monitoring and audits.\nContribute to the evolution of the BI data platform by applying best practices in data modeling, orchestration, and governance.\nInvestigate and resolve complex data issues in partnership with cross-function teams.\nQualifications\nBachelor’s degree in Computer Science, Information Systems, Engineering, Data Science, Mathematics, or related field.\n2+ years of experience developing and maintaining data pipelines in cloud environments.\nStrong SQL and Python skills for large-scale data analysis and engineering.\nExperience working with business stakeholders to gather requirements and deliver end-to-end solutions.\nGood communication and organization skills\nNice to Have\nExperience with GCP, including BigQuery, Cloud Storage, Datastream, and Pub/Sub\nExperience with orchestration tools (Airflow) and data modeling tools (dbt).\nFamiliarity with BI/visualization platforms (Looker) and APIs/SaaS integrations\nExposure to CI/CD pipelines or automated testing for data workflows\nCHI: $70,000-90,000\nThe expected salary range above is applicable if the role is performed from Illinois and may vary for other locations (California, Colorado, New York). Actual salary may vary based on qualifications and experience. Tempus offers a full range of benefits, which may include incentive compensation, restricted stock units, medical and other benefits depending on the position.\nWe are an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n\n\n\n\n\n\n\n        \n            Show more\n          \n\n          \n\n\n\n\n\n\n\n        \n            Show less",
        "skills": [
          "SQL",
          "Python",
          "Looker",
          "Airflow",
          "GCP"
        ],
        "industry": "Not specified"
      }
    },
    {
      "title": "Data Engineer II - QuantumBlack, AI by McKinsey",
      "company": "QuantumBlack, AI by McKinsey",
      "location": "New York, NY",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-ii-quantumblack-ai-by-mckinsey-at-quantumblack-ai-by-mckinsey-4316347719",
      "post_date": "2025-10-17",
      "scraped_at": "2025-10-21T13:32:40.890415",
      "source": "public_api"
    },
    {
      "title": "Data Engineer, Analytics",
      "company": "OpenAI",
      "location": "San Francisco, CA",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-analytics-at-openai-4313819229",
      "post_date": "2025-10-18",
      "scraped_at": "2025-10-21T13:32:46.833942",
      "source": "public_api"
    },
    {
      "title": "Data Engineer Intern",
      "company": "Qualer, Now a MasterControl Company",
      "location": "Salt Lake City, UT",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-intern-at-qualer-now-a-mastercontrol-company-4315276519",
      "post_date": "2025-10-16",
      "scraped_at": "2025-10-21T13:32:46.833942",
      "source": "public_api"
    },
    {
      "title": "Software Engineer, Backend",
      "company": "Tinder",
      "location": "Los Angeles, CA",
      "url": "https://www.linkedin.com/jobs/view/software-engineer-backend-at-tinder-4307089715",
      "post_date": "2025-10-18",
      "scraped_at": "2025-10-21T13:32:46.833942",
      "source": "public_api"
    },
    {
      "title": "Software Engineer, AI Platform",
      "company": "LinkedIn",
      "location": "Mountain View, CA",
      "url": "https://www.linkedin.com/jobs/view/software-engineer-ai-platform-at-linkedin-4317358470",
      "post_date": null,
      "scraped_at": "2025-10-21T13:32:46.833942",
      "source": "public_api"
    },
    {
      "title": "Data Engineer I",
      "company": "Les Schwab Tire Centers",
      "location": "Bend, OR",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-i-at-les-schwab-tire-centers-4316669910",
      "post_date": null,
      "scraped_at": "2025-10-21T13:32:46.833942",
      "source": "public_api"
    },
    {
      "title": "Data Engineer",
      "company": "Hyundai Capital America",
      "location": "Irvine, CA",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-hyundai-capital-america-4316368790",
      "post_date": "2025-10-17",
      "scraped_at": "2025-10-21T13:32:46.833942",
      "source": "public_api"
    },
    {
      "title": "Data Engineer 5 - Product (QOE)",
      "company": "Netflix",
      "location": "United States",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-5-product-qoe-at-netflix-4293138779",
      "post_date": "2025-10-16",
      "scraped_at": "2025-10-21T13:32:46.833942",
      "source": "public_api"
    },
    {
      "title": "Data Engineer - Business Strategy Insights Analytics",
      "company": "Spotify",
      "location": "New York, NY",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-business-strategy-insights-analytics-at-spotify-4306273737",
      "post_date": "2025-10-17",
      "scraped_at": "2025-10-21T13:32:46.833942",
      "source": "public_api"
    },
    {
      "title": "Junior Data Engineer",
      "company": "Lensa",
      "location": "McLean, VA",
      "url": "https://www.linkedin.com/jobs/view/junior-data-engineer-at-lensa-4316857596",
      "post_date": "2025-10-19",
      "scraped_at": "2025-10-21T13:32:46.833942",
      "source": "public_api"
    },
    {
      "title": "Sr. Data Engineer",
      "company": "Phoenix Suns",
      "location": "Phoenix, AZ",
      "url": "https://www.linkedin.com/jobs/view/sr-data-engineer-at-phoenix-suns-4317541068",
      "post_date": "2025-10-20",
      "scraped_at": "2025-10-21T13:32:46.833942",
      "source": "public_api"
    },
    {
      "title": "Data Engineer",
      "company": "FLINT",
      "location": "Roseville, CA",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-flint-4314806130",
      "post_date": "2025-10-14",
      "scraped_at": "2025-10-21T13:32:52.782371",
      "source": "public_api"
    },
    {
      "title": "Data Engineer",
      "company": "Temu",
      "location": "United States",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-temu-4316691384",
      "post_date": null,
      "scraped_at": "2025-10-21T13:32:52.782371",
      "source": "public_api"
    },
    {
      "title": "Data Engineer",
      "company": "LTIMindtree",
      "location": "Phoenix, AZ",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-ltimindtree-4314204046",
      "post_date": "2025-10-13",
      "scraped_at": "2025-10-21T13:32:52.782371",
      "source": "public_api"
    },
    {
      "title": "Software Engineer, Database Systems",
      "company": "OpenAI",
      "location": "San Francisco, CA",
      "url": "https://www.linkedin.com/jobs/view/software-engineer-database-systems-at-openai-4268364964",
      "post_date": "2025-10-12",
      "scraped_at": "2025-10-21T13:32:52.782371",
      "source": "public_api"
    },
    {
      "title": "Data/Visualization Engineer 4",
      "company": "Nike",
      "location": "Beaverton, OR",
      "url": "https://www.linkedin.com/jobs/view/data-visualization-engineer-4-at-nike-4316907364",
      "post_date": null,
      "scraped_at": "2025-10-21T13:32:52.782371",
      "source": "public_api"
    },
    {
      "title": "Data Engineer/Sr Data Engineer, IT Analytics",
      "company": "American Airlines",
      "location": "Dallas, TX",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-sr-data-engineer-it-analytics-at-american-airlines-4314944973",
      "post_date": null,
      "scraped_at": "2025-10-21T13:32:52.782371",
      "source": "public_api"
    },
    {
      "title": "Data Engineer",
      "company": "Caesars Entertainment",
      "location": "United States",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-caesars-entertainment-4313067643",
      "post_date": "2025-10-16",
      "scraped_at": "2025-10-21T13:32:52.782371",
      "source": "public_api"
    },
    {
      "title": "Data Engineer",
      "company": "City of McKinney",
      "location": "McKinney, TX",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-city-of-mckinney-4313300960",
      "post_date": "2025-10-17",
      "scraped_at": "2025-10-21T13:32:52.782371",
      "source": "public_api"
    },
    {
      "title": "Data Engineer Intern (Early Career)",
      "company": "S&P Global",
      "location": "New York, NY",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-intern-early-career-at-s-p-global-4315759480",
      "post_date": "2025-10-17",
      "scraped_at": "2025-10-21T13:32:52.782371",
      "source": "public_api"
    },
    {
      "title": "Junior Data Engineer",
      "company": "Medpace",
      "location": "Cincinnati, OH",
      "url": "https://www.linkedin.com/jobs/view/junior-data-engineer-at-medpace-4217695016",
      "post_date": "2025-10-16",
      "scraped_at": "2025-10-21T13:32:52.784376",
      "source": "public_api"
    },
    {
      "title": "Data Engineer(SQl, Python)",
      "company": "Tata Consultancy Services",
      "location": "Sunnyvale, CA",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-sql-python-at-tata-consultancy-services-4314544915",
      "post_date": "2025-10-14",
      "scraped_at": "2025-10-21T13:32:58.648666",
      "source": "public_api"
    },
    {
      "title": "Data Engineer - Analytics and AI Platforms",
      "company": "Sunlighten",
      "location": "Leawood, KS",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-analytics-and-ai-platforms-at-sunlighten-4317383998",
      "post_date": null,
      "scraped_at": "2025-10-21T13:32:58.648666",
      "source": "public_api"
    },
    {
      "title": "Data Engineer II, Ring Data Warehouse",
      "company": "Amazon",
      "location": "Hawthorne, CA",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-ii-ring-data-warehouse-at-amazon-4316533837",
      "post_date": "2025-10-18",
      "scraped_at": "2025-10-21T13:32:58.648666",
      "source": "public_api"
    },
    {
      "title": "Data Engineer, Python, AWS, Databricks",
      "company": "JPMorganChase",
      "location": "Jersey City, NJ",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-python-aws-databricks-at-jpmorganchase-4292998358",
      "post_date": "2025-10-17",
      "scraped_at": "2025-10-21T13:32:58.648666",
      "source": "public_api"
    },
    {
      "title": "Data-Senior Data Engineer–CL",
      "company": "Endava",
      "location": "Deerfield, IL",
      "url": "https://www.linkedin.com/jobs/view/data-senior-data-engineer%E2%80%93cl-at-endava-4313099166",
      "post_date": "2025-10-16",
      "scraped_at": "2025-10-21T13:32:58.652174",
      "source": "public_api"
    },
    {
      "title": "Data Engineer",
      "company": "Saragossa",
      "location": "Chicago, IL",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-saragossa-4314917161",
      "post_date": null,
      "scraped_at": "2025-10-21T13:32:58.652174",
      "source": "public_api"
    },
    {
      "title": "Data Engineer",
      "company": "Montegallo.",
      "location": "United States",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-at-montegallo-4316381737",
      "post_date": "2025-10-17",
      "scraped_at": "2025-10-21T13:32:58.652174",
      "source": "public_api"
    },
    {
      "title": "Senior Data Engineer",
      "company": "Fanatics",
      "location": "United States",
      "url": "https://www.linkedin.com/jobs/view/senior-data-engineer-at-fanatics-4313313023",
      "post_date": "2025-10-17",
      "scraped_at": "2025-10-21T13:32:58.652174",
      "source": "public_api"
    },
    {
      "title": "Data Engineer I (AWS, Python/ SQL and Data Bricks)",
      "company": "Travelers",
      "location": "Hartford, CT",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-i-aws-python-sql-and-data-bricks-at-travelers-4315099027",
      "post_date": "2025-10-16",
      "scraped_at": "2025-10-21T13:32:58.652174",
      "source": "public_api"
    },
    {
      "title": "Data Engineer - IT Sustainability",
      "company": "Costco Wholesale",
      "location": "Seattle, WA",
      "url": "https://www.linkedin.com/jobs/view/data-engineer-it-sustainability-at-costco-wholesale-4316379563",
      "post_date": "2025-10-17",
      "scraped_at": "2025-10-21T13:32:58.652174",
      "source": "public_api"
    }
  ],
  "metadata": {
    "scraped_at": "2025-10-21T13:35:03.973300",
    "total_jobs": 50,
    "total_with_details": 15
  }
}